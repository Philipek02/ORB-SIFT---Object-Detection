{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa17b08a",
   "metadata": {},
   "source": [
    "# SIFT object recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93a3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV 4.11.0\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7, 4)\n",
    "print('OpenCV', cv2.__version__)\n",
    "\n",
    "# === paths ===\n",
    "keys_dir   = Path('dataset/keys')\n",
    "video_path = Path('dataset/video.mp4')\n",
    "output_dir = Path('results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === hyper‑params ===\n",
    "FRAME_SCALE      = 0.5         # 0.5 → szer. ≈ 640 px przy 1280×720\n",
    "CUDA_SIFT_PARAMS = dict(nfeatures=800, contrastThreshold=0.04,\n",
    "                        edgeThreshold=10, nOctaveLayers=3)\n",
    "\n",
    "RATIO_TEST       = 0.75\n",
    "MIN_INLIERS      = 10\n",
    "MIN_INLIER_RATIO = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_upload(img_bgr, blur=False):\n",
    "    \"\"\"Skalowanie, konwersja do gray, (opcjonalnie blur), upload na GPU.\"\"\"\n",
    "    small = cv2.resize(img_bgr, None, fx=FRAME_SCALE, fy=FRAME_SCALE,\n",
    "                       interpolation=cv2.INTER_AREA)\n",
    "    gray  = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "    if blur:\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    gpu   = cv2.cuda_GpuMat()\n",
    "    gpu.upload(gray)\n",
    "    return gpu, gray.shape  # shape w zeskalowanej przestrzeni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929dbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detector():\n",
    "    return cv2.cuda.SIFT_create(**CUDA_SIFT_PARAMS)\n",
    "\n",
    "def create_flann_matcher():\n",
    "    index_params = dict(algorithm=1, trees=5)     # KD‑Tree\n",
    "    search_params = dict(checks=30)\n",
    "    return cv2.FlannBasedMatcher(index_params, search_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b51a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_key_images(detector, folder):\n",
    "    db = []\n",
    "    for path in sorted(folder.iterdir()):\n",
    "        if path.suffix.lower() not in {'.jpg', '.jpeg', '.png', '.bmp'}:\n",
    "            continue\n",
    "        img_bgr = cv2.imread(str(path))\n",
    "        gpu, shape = preprocess_and_upload(img_bgr)\n",
    "        kp_gpu, des_gpu = detector.detectAndComputeAsync(gpu, None)\n",
    "        kp   = detector.convert(kp_gpu)\n",
    "        des  = des_gpu.download()                  # float32 (128D)\n",
    "        if des is not None:\n",
    "            db.append({'name': path.stem,\n",
    "                       'kp': kp,\n",
    "                       'des': des,\n",
    "                       'shape': shape})\n",
    "    print('Loaded', len(db), 'key images')\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb34f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_matches(matcher, d_query, d_train):\n",
    "    if d_query is None or d_train is None:\n",
    "        return []\n",
    "    knn = matcher.knnMatch(d_query, d_train, k=2)\n",
    "    good = []\n",
    "    for pair in knn:\n",
    "        if len(pair) == 2:\n",
    "            m, n = pair\n",
    "            if m.distance < RATIO_TEST * n.distance:\n",
    "                good.append(m)\n",
    "    return good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d6776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_consistency_filter(matches, kp2, radius=100, min_neighbors=3):\n",
    "    \"\"\"Dodatkowy filtr każde dopasowanie zachowujemy, jeśli w promieniu\n",
    "    `radius` pikseli ma co najmniej `min_neighbors` innych punktów.\"\"\"\n",
    "    filtered = []\n",
    "    for m in matches:\n",
    "        pt2 = np.array(kp2[m.trainIdx].pt)\n",
    "        neighbors = [mm for mm in matches\n",
    "                     if np.linalg.norm(np.array(kp2[mm.trainIdx].pt) - pt2) < radius]\n",
    "        if len(neighbors) > min_neighbors:\n",
    "            filtered.append(m)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414d7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(frame, box, label):\n",
    "    thickness = max(2, int(0.004 * frame.shape[1]))\n",
    "    cv2.polylines(frame, [np.int32(box)], True, (255, 0, 0),\n",
    "                  thickness, cv2.LINE_AA)\n",
    "    cv2.putText(frame, label, (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                (255, 0, 0), 2, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7b9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video():\n",
    "    det     = create_detector()\n",
    "    matcher = create_flann_matcher()\n",
    "    key_db  = load_key_images(det, keys_dir)\n",
    "    print('Loaded', len(key_db), 'key images')\n",
    "\n",
    "    cap   = cv2.VideoCapture(str(video_path))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or None\n",
    "    fps   = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    w, h  = int(cap.get(3)), int(cap.get(4))\n",
    "    writer = cv2.VideoWriter(str(output_dir/'sift.mp4'),\n",
    "                             cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "\n",
    "    stats = defaultdict(int)\n",
    "    pbar  = tqdm(total=total, desc='video', unit='frame')\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray_f = preprocess(frame, blur=False)\n",
    "        kp_f, des_f = det.detectAndCompute(gray_f, None)\n",
    "\n",
    "        best = None\n",
    "        best_matches = []\n",
    "        best_H = None\n",
    "\n",
    "        for db in key_db:\n",
    "            gm = good_matches(matcher, db['des'], des_f)\n",
    "            gm = spatial_consistency_filter(gm, kp_f)  # ekstra filtr\n",
    "            if len(gm) < MIN_INLIERS:\n",
    "                continue\n",
    "\n",
    "            src = np.float32([db['kp'][m.queryIdx].pt for m in gm]).reshape(-1,1,2)\n",
    "            dst = np.float32([kp_f[m.trainIdx].pt for m in gm]).reshape(-1,1,2)\n",
    "            H, mask = cv2.findHomography(src, dst, cv2.RANSAC, 5.0)\n",
    "            if H is None:\n",
    "                continue\n",
    "\n",
    "            inliers = mask.ravel().sum()\n",
    "            if inliers >= MIN_INLIERS and inliers / len(gm) >= MIN_INLIER_RATIO:\n",
    "                if inliers > len(best_matches):\n",
    "                    best = db\n",
    "                    best_matches = gm\n",
    "                    best_H = H\n",
    "\n",
    "        if best is not None:\n",
    "            h0, w0 = best['shape']\n",
    "            box = cv2.perspectiveTransform(\n",
    "                np.float32([[0,0], [w0,0], [w0,h0], [0,h0]]).reshape(-1,1,2),\n",
    "                best_H)\n",
    "            annotate(frame, box, f\"{best['name']} ({len(best_matches)})\")\n",
    "            stats[best['name']] += len(best_matches)\n",
    "\n",
    "        writer.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close(); cap.release(); writer.release()\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de8e3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 key images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2a9dbd53c14296b22d76e3cd1d85f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "video:   0%|          | 0/763 [00:00<?, ?frame/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stats = \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m db \u001b[38;5;129;01min\u001b[39;00m key_db:\n\u001b[32m     30\u001b[39m     gm = good_matches(matcher, db[\u001b[33m'\u001b[39m\u001b[33mdes\u001b[39m\u001b[33m'\u001b[39m], des_f)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     gm = \u001b[43mspatial_consistency_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_f\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ekstra filtr\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(gm) < MIN_INLIERS:\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mspatial_consistency_filter\u001b[39m\u001b[34m(matches, kp2, radius, min_neighbors)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[32m      6\u001b[39m     pt2 = np.array(kp2[m.trainIdx].pt)\n\u001b[32m      7\u001b[39m     neighbors = [mm \u001b[38;5;28;01mfor\u001b[39;00m mm \u001b[38;5;129;01min\u001b[39;00m matches\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m                  \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkp2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainIdx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt2\u001b[49m\u001b[43m)\u001b[49m < radius]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neighbors) > min_neighbors:\n\u001b[32m     10\u001b[39m         filtered.append(m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakub\\Pliki\\Studia\\Sem 6\\WMA\\ORB i SIFT\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2744\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2742\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2743\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2744\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2745\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "stats = process_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6086426",
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats:\n",
    "    names  = list(stats.keys())\n",
    "    counts = [stats[n] for n in names]\n",
    "    plt.bar(names, counts)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Cumulative matches')\n",
    "    plt.title('Good matches per key image (SIFT)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(str(output_dir/'sift.mp4'))\n",
    "ret, fr = cap.read(); cap.release()\n",
    "if ret:\n",
    "    fr_rgb = cv2.cvtColor(fr, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(fr_rgb)\n",
    "    plt.title('Annotated frame sample (SIFT)')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
